{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm import tqdm_notebook\n",
    "%matplotlib inline \n",
    "# from google.colab.patches import cv2_imshow\n",
    "from IPython.display import HTML #imports to play videos\n",
    "from base64 import b64encode \n",
    "import cv2 as cv\n",
    "#from skimage.measure import compare_ssim\n",
    "import glob\n",
    "import time\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1, extract_face\n",
    "from tqdm import tqdm\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import math\n",
    "import pickle\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "import skimage.measure\n",
    "import albumentations as A\n",
    "from tqdm.notebook import tqdm \n",
    "#from albumentations.pytorch import ToTensor \n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models.video import mc3_18, r2plus1d_18\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "import sys\n",
    "import sklearn\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "SOURCE_FOLDER = r\"D:\\dfdc\"\n",
    "TRAIN_FOLDER = os.path.join(SOURCE_FOLDER, 'dfdc_videos', \"dfdc_train_part_1\") \n",
    "TEST_FOLDER = os.path.join(SOURCE_FOLDER, 'dfdc_videos', \"dfdc_train_part_0\") \n",
    "\n",
    "TRAIN_FOLDER_CROPS = os.path.join(SOURCE_FOLDER, 'crops', \"dfdc_train_part_1\") \n",
    "TEST_FOLDER_CROPS = os.path.join(SOURCE_FOLDER, 'crops', \"dfdc_train_part_0\") \n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json\n",
    "def get_videos_from_folder(folder_path):\n",
    "    # List all files in the specified folder\n",
    "    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    if 'metadata.json' in files:\n",
    "        files.remove('metadata.json')\n",
    "    full_paths = [os.path.join(folder_path, f) for f in files]\n",
    "    return full_paths\n",
    "\n",
    "def get_videos_basenames_from_folder(folder_path):\n",
    "    # List all files in the specified folder\n",
    "    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    if 'metadata.json' in files:\n",
    "        files.remove('metadata.json')\n",
    "    return files\n",
    "\n",
    "def get_original_with_fakes(root_dir):\n",
    "    pairs = []\n",
    "    for json_path in glob(os.path.join(root_dir, \"*/metadata.json\")):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "        for k, v in metadata.items():\n",
    "            original = v.get(\"original\", None)\n",
    "            if v[\"label\"] == \"FAKE\":\n",
    "                pairs.append((original[:-4], k[:-4] ))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "metadata_val = pd.read_json(TEST_FOLDER+ '/metadata.json')\n",
    "metadata_train = pd.read_json(TRAIN_FOLDER+ '/metadata.json')\n",
    "#metadata['aaqaifqrwn.mp4']['label']\n",
    "\n",
    "def sample_weights(metadata):\n",
    "    video_fake_real_labels = metadata.iloc[0]\n",
    "    class_counts = video_fake_real_labels.value_counts()\n",
    "    class_weights = 1 / class_counts\n",
    "    return [1/class_counts[i] for i in video_fake_real_labels] # for undersampling\n",
    "\n",
    "train_weights_sampler = WeightedRandomSampler(weights=sample_weights(metadata_train), num_samples=len(metadata_train.iloc[0]), replacement=True)\n",
    "val_weights_sampler =WeightedRandomSampler(weights=sample_weights(metadata_val), num_samples=len(metadata_val.iloc[0]), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set will be 00\n",
    "metadata_val = pd.read_json(TEST_FOLDER+ '/metadata.json')\n",
    "metadata_train = pd.read_json(TRAIN_FOLDER+ '/metadata.json')\n",
    "video_fake_real_labels = metadata_train.iloc[0]\n",
    "real_df = video_fake_real_labels[video_fake_real_labels == 'REAL'].keys()\n",
    "fake_df = video_fake_real_labels[video_fake_real_labels == 'REAL'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, metadata, device, mode='train'):\n",
    "        self.device = device\n",
    "        self.folder_path = TRAIN_FOLDER if mode=='train' else TEST_FOLDER\n",
    "        self.folder_path_crops = TRAIN_FOLDER_CROPS if mode=='train' else TEST_FOLDER_CROPS\n",
    "        self.video_fake_real_labels = metadata.iloc[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_fake_real_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.video_fake_real_labels.index[idx]\n",
    "        label = 0 if self.video_fake_real_labels[filename] == 'REAL' else 1\n",
    "        crops_folder_path = os.path.join(self.folder_path_crops, filename[:-4])\n",
    "        crops_list = [os.path.join(crops_folder_path, f) for f in os.listdir(crops_folder_path) if os.path.isfile(os.path.join(crops_folder_path, f))]\n",
    "        # get crops from folder\n",
    "        crop_tensors = []\n",
    "        \n",
    "        for crop_file in crops_list:\n",
    "            loaded_tensor = torch.load(crop_file)\n",
    "            crop_tensors.append(loaded_tensor)\n",
    "        \n",
    "        crop_tensors = torch.stack(crop_tensors, dim=0) # stack tensors. this is a \"batch\"\n",
    "        labels = torch.tensor([label for i in range(len(crops_list))]) # label for each frame is the same as video label\n",
    "\n",
    "        return crop_tensors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "#pretrained_weights_path = 'noisy-student-efficientnet-b4.pth' # noisy student\n",
    "#model = EfficientNet.from_name('efficientnet-b4')\n",
    "#model = EfficientNet.from_name('efficientnet-b1')\n",
    "#model.load_state_dict(torch.load(pretrained_weights_path, map_location=torch.device(device)))\n",
    "#model._fc = nn.Linear(in_features=model._fc.in_features, out_features=1)\n",
    "class EffnetTest(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EffnetTest, self).__init__()\n",
    "        self.model = EfficientNet.from_pretrained(f\"efficientnet-b0\", num_classes=1)\n",
    "        #self.model._norm_layer = nn.GroupNorm(num_groups=32, num_channels=3)\n",
    "        #for i in range(4):\n",
    "        #    for param in self.model._blocks[i].parameters():\n",
    "        #        param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return torch.sigmoid(x).squeeze()\n",
    "model = EffnetTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "├─EfficientNet: 1-1                                     [-1, 1]                   --\n",
      "|    └─Conv2dStaticSamePadding: 2-1                     [-1, 32, 112, 112]        --\n",
      "|    |    └─ZeroPad2d: 3-1                              [-1, 3, 225, 225]         --\n",
      "|    └─BatchNorm2d: 2-2                                 [-1, 32, 112, 112]        64\n",
      "|    └─MemoryEfficientSwish: 2-3                        [-1, 32, 112, 112]        --\n",
      "|    └─ModuleList: 2                                    []                        --\n",
      "|    |    └─MBConvBlock: 3-2                            [-1, 16, 112, 112]        1,448\n",
      "|    |    └─MBConvBlock: 3-3                            [-1, 24, 56, 56]          6,004\n",
      "|    |    └─MBConvBlock: 3-4                            [-1, 24, 56, 56]          10,710\n",
      "|    |    └─MBConvBlock: 3-5                            [-1, 40, 28, 28]          15,350\n",
      "|    |    └─MBConvBlock: 3-6                            [-1, 40, 28, 28]          31,290\n",
      "|    |    └─MBConvBlock: 3-7                            [-1, 80, 14, 14]          37,130\n",
      "|    |    └─MBConvBlock: 3-8                            [-1, 80, 14, 14]          102,900\n",
      "|    |    └─MBConvBlock: 3-9                            [-1, 80, 14, 14]          102,900\n",
      "|    |    └─MBConvBlock: 3-10                           [-1, 112, 14, 14]         126,004\n",
      "|    |    └─MBConvBlock: 3-11                           [-1, 112, 14, 14]         208,572\n",
      "|    |    └─MBConvBlock: 3-12                           [-1, 112, 14, 14]         208,572\n",
      "|    |    └─MBConvBlock: 3-13                           [-1, 192, 7, 7]           262,492\n",
      "|    |    └─MBConvBlock: 3-14                           [-1, 192, 7, 7]           587,952\n",
      "|    |    └─MBConvBlock: 3-15                           [-1, 192, 7, 7]           587,952\n",
      "|    |    └─MBConvBlock: 3-16                           [-1, 192, 7, 7]           587,952\n",
      "|    |    └─MBConvBlock: 3-17                           [-1, 320, 7, 7]           717,232\n",
      "|    └─Conv2dStaticSamePadding: 2-4                     [-1, 1280, 7, 7]          --\n",
      "|    |    └─Identity: 3-18                              [-1, 320, 7, 7]           --\n",
      "|    └─BatchNorm2d: 2-5                                 [-1, 1280, 7, 7]          2,560\n",
      "|    └─MemoryEfficientSwish: 2-6                        [-1, 1280, 7, 7]          --\n",
      "|    └─AdaptiveAvgPool2d: 2-7                           [-1, 1280, 1, 1]          --\n",
      "|    └─Dropout: 2-8                                     [-1, 1280]                --\n",
      "|    └─Linear: 2-9                                      [-1, 1]                   1,281\n",
      "=========================================================================================================\n",
      "Total params: 3,598,365\n",
      "Trainable params: 3,598,365\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 392.10\n",
      "=========================================================================================================\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 51.40\n",
      "Params size (MB): 13.73\n",
      "Estimated Total Size (MB): 65.71\n",
      "=========================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "├─EfficientNet: 1-1                                     [-1, 1]                   --\n",
       "|    └─Conv2dStaticSamePadding: 2-1                     [-1, 32, 112, 112]        --\n",
       "|    |    └─ZeroPad2d: 3-1                              [-1, 3, 225, 225]         --\n",
       "|    └─BatchNorm2d: 2-2                                 [-1, 32, 112, 112]        64\n",
       "|    └─MemoryEfficientSwish: 2-3                        [-1, 32, 112, 112]        --\n",
       "|    └─ModuleList: 2                                    []                        --\n",
       "|    |    └─MBConvBlock: 3-2                            [-1, 16, 112, 112]        1,448\n",
       "|    |    └─MBConvBlock: 3-3                            [-1, 24, 56, 56]          6,004\n",
       "|    |    └─MBConvBlock: 3-4                            [-1, 24, 56, 56]          10,710\n",
       "|    |    └─MBConvBlock: 3-5                            [-1, 40, 28, 28]          15,350\n",
       "|    |    └─MBConvBlock: 3-6                            [-1, 40, 28, 28]          31,290\n",
       "|    |    └─MBConvBlock: 3-7                            [-1, 80, 14, 14]          37,130\n",
       "|    |    └─MBConvBlock: 3-8                            [-1, 80, 14, 14]          102,900\n",
       "|    |    └─MBConvBlock: 3-9                            [-1, 80, 14, 14]          102,900\n",
       "|    |    └─MBConvBlock: 3-10                           [-1, 112, 14, 14]         126,004\n",
       "|    |    └─MBConvBlock: 3-11                           [-1, 112, 14, 14]         208,572\n",
       "|    |    └─MBConvBlock: 3-12                           [-1, 112, 14, 14]         208,572\n",
       "|    |    └─MBConvBlock: 3-13                           [-1, 192, 7, 7]           262,492\n",
       "|    |    └─MBConvBlock: 3-14                           [-1, 192, 7, 7]           587,952\n",
       "|    |    └─MBConvBlock: 3-15                           [-1, 192, 7, 7]           587,952\n",
       "|    |    └─MBConvBlock: 3-16                           [-1, 192, 7, 7]           587,952\n",
       "|    |    └─MBConvBlock: 3-17                           [-1, 320, 7, 7]           717,232\n",
       "|    └─Conv2dStaticSamePadding: 2-4                     [-1, 1280, 7, 7]          --\n",
       "|    |    └─Identity: 3-18                              [-1, 320, 7, 7]           --\n",
       "|    └─BatchNorm2d: 2-5                                 [-1, 1280, 7, 7]          2,560\n",
       "|    └─MemoryEfficientSwish: 2-6                        [-1, 1280, 7, 7]          --\n",
       "|    └─AdaptiveAvgPool2d: 2-7                           [-1, 1280, 1, 1]          --\n",
       "|    └─Dropout: 2-8                                     [-1, 1280]                --\n",
       "|    └─Linear: 2-9                                      [-1, 1]                   1,281\n",
       "=========================================================================================================\n",
       "Total params: 3,598,365\n",
       "Trainable params: 3,598,365\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 392.10\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.57\n",
       "Forward/backward pass size (MB): 51.40\n",
       "Params size (MB): 13.73\n",
       "Estimated Total Size (MB): 65.71\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, (3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=5, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = float('inf')  # Initialize to positive infinity\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def early_stop(self, val_loss, model):\n",
    "        if val_loss < self.best_score:\n",
    "            self.best_score = val_loss  # Update best validation loss\n",
    "            self.counter = 0  # Reset patience counter\n",
    "            torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "        else:\n",
    "            self.counter += 1  # Increment patience counter\n",
    "           \n",
    "        if self.counter >= self.patience:\n",
    "            if self.verbose:\n",
    "                print(\"Early stopping...\")\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DeepfakeDataset(metadata_train, device, mode='train')\n",
    "train_loader = DataLoader(train_dataset, shuffle=False, sampler=train_weights_sampler, batch_size=1, num_workers=0) # process video 1 by 1\n",
    "val_dataset = DeepfakeDataset(metadata_val, device, mode='val')\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, sampler=val_weights_sampler, batch_size=1, num_workers=0) # process video 1 by 1\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_=48, pin_memory=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "train_loss_history = []\n",
    "train_accuracy_history = []\n",
    "val_loss_history = []\n",
    "val_accuracy_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sheryl\\AppData\\Local\\Temp\\ipykernel_10328\\3301423534.py:20: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7471, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6435, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6465, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6432, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6478, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7528, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7549, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6444, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6512, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6341, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7643, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6518, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7558, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6503, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6351, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7707, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7543, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6395, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6491, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7617, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7423, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6481, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6452, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6535, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6435, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6372, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6324, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7501, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6396, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7499, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6423, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6436, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7624, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6334, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6444, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6326, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7565, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7452, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6334, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6425, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6315, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7706, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7491, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7606, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6402, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7468, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6475, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6387, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6381, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7527, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6465, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6391, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6553, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7550, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7522, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7501, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6379, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7714, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6373, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7676, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7568, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6283, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7673, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7634, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6362, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6415, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7581, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7548, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6438, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7513, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7639, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6364, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6412, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7372, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7557, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6412, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7307, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6414, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7336, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7564, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7600, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6477, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7475, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6364, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7415, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7437, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7514, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7421, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6490, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6309, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6386, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7481, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6467, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6346, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7406, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m predicted \u001b[38;5;241m=\u001b[39m (outputs \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;66;03m# calculate if label is 0 or 1\u001b[39;00m\n\u001b[0;32m     24\u001b[0m losses\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 25\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     26\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(losses)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "model = model.to(device)\n",
    "earlystopper = EarlyStopper()\n",
    "# mtcnn unable to detect gan/diffusion faces\n",
    "#data = data.to(device)  # Move data to GPU if available\n",
    "# half precision\n",
    "# A6000 at desktop\n",
    "for epoch in range(epochs): \n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for data, labels in train_loader:\n",
    "        data = data.squeeze(0)\n",
    "        labels = labels.squeeze(0)\n",
    "        labels = labels.to(device).float()\n",
    "        optimizer.zero_grad()  # clear previous gradients\n",
    "\n",
    "        data = data.to(device).float()\n",
    "        outputs = model(data)\n",
    "        losses = loss_fn(outputs, labels)\n",
    "        running_loss += losses.item()  # accumulate loss\n",
    "        predicted = (outputs >= 0.5).float() # calculate if label is 0 or 1\n",
    "        losses.backward()\n",
    "        correct += (predicted == labels).sum().item() \n",
    "        total += labels.size(0)\n",
    "        print(losses)\n",
    "\n",
    "    average_train_loss = running_loss / total\n",
    "    average_train_accuracy = correct / total\n",
    "    train_loss_history.append(average_train_loss)\n",
    "    train_accuracy_history.append(average_train_accuracy)\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0 \n",
    "    for data, labels in val_loader:\n",
    "        data = data.squeeze(0)\n",
    "        labels = labels.squeeze(0)\n",
    "        labels = labels.to(device).float()\n",
    "        data = data.to(device).float()\n",
    "\n",
    "        outputs = model(data)\n",
    "        losses = loss_fn(outputs, labels)\n",
    "        running_loss += losses.item()  # accumulate loss\n",
    "        predicted = (outputs >= 0.5).float() # calculate if label is 0 or 1\n",
    "        losses.backward()\n",
    "        correct += (predicted == labels).sum().item() \n",
    "        total += labels.size(0)\n",
    "    average_val_loss = running_loss / total\n",
    "    average_val_accuracy = correct / total\n",
    "    val_loss_history.append(average_val_loss)\n",
    "    val_accuracy_history.append(average_val_accuracy)\n",
    "    print(f\"Epoch [{epoch}/{epochs}], Train Loss: {average_train_loss:.4f}, Train Accuracy: {average_train_accuracy:.4f}, Validation Loss: {average_val_loss:.4f}, Validation Accuracy: {average_val_accuracy:.4f}\")\n",
    "    #if earlystopper.early_stop(average_val_loss, model):\n",
    "    #    break \n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained_weights_path = 'noisy-student-efficientnet-b4.pth' # noisy student\n",
    "#model = EfficientNet.from_name('efficientnet-b4')\n",
    "#model.load_state_dict(torch.load(pretrained_weights_path, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
