{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea1e6230-1dd8-4897-a8f6-2995143f5c83",
   "metadata": {},
   "source": [
    "## Memory constraint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2286ea2d-7065-45e1-abbc-8ba6eb113bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tf.test.gpu_device_name()\n",
    "# #Run on GPU\n",
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if len(physical_devices) > 0:\n",
    "#     tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    \n",
    "#Run on CPU\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "# # Create some tensors and perform an operation\n",
    "# a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "# b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "# c = tf.matmul(a, b)\n",
    "\n",
    "# print(c)\n",
    "# tf.debugging.set_log_device_placement(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9839169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47745e5f-dc50-4d80-800d-9e6abdba7f79",
   "metadata": {},
   "source": [
    "### Load & Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144b9546-7fef-4acd-bb03-fccced7da370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def load_dataset():\n",
    "    # load dataset\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    # one hot encode target values\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bef26ef-6f54-45ec-873d-3068a06df74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_pixels(train, test):\n",
    "    # convert from integers to floats\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # normalize to range 0-1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # return normalized images\n",
    "    return train_norm, test_norm\n",
    "\n",
    "X_train , X_test = prep_pixels(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f540f9e1-4aac-4e4a-badf-c6f0dd748a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients(image_list, label_list, num_clients=10, initial='clients'):\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "    #randomize the data\n",
    "    data = list(zip(image_list, label_list))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    #shard data and place at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))}\n",
    "\n",
    "def batch_data(data_shard, bs=32):\n",
    "    '''Takes in a clients data shard and create a tfds object off it\n",
    "    args:\n",
    "        shard: a data, label constituting a client's data shard\n",
    "        bs:batch size\n",
    "    return:\n",
    "        tfds object'''\n",
    "    #seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)\n",
    "\n",
    "\n",
    "clients = create_clients(X_train, y_train, num_clients=10, initial='client')\n",
    "\n",
    "#process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "    \n",
    "#process and batch the test set  \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566fb11d-d013-4f00-b763-10e933e78f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "for i in range(2):\n",
    "    img = X_test[i].reshape((32,32,3))\n",
    "    plt.imshow(img, cmap=\"Greys\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960271b9-7a50-42d8-a787-db13fe5532ae",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61af8fcd-22ef-482e-afa3-30852aeed001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "# from keras import Sequential\n",
    "# from keras import applications\n",
    "\n",
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build(shape, classes,only_digits=True):\n",
    "        base_model_1 = VGG16(include_top=False,input_shape=(32,32,3),classes=y_train.shape[1])\n",
    "        model_1= Sequential()\n",
    "        model_1.add(base_model_1) #Adds the base model (in this case vgg19 to model_1)\n",
    "        model_1.add(Flatten()) #Since the output before the flatten layer is a matrix we have to use this function to get a vector of the form nX1 to feed it into the fully connected layers\n",
    "        #Add the Dense layers along with activation and batch normalization\n",
    "        model_1.add(Dense(1024,activation=('relu'),input_dim=512))\n",
    "        model_1.add(Dense(512,activation=('relu'))) \n",
    "        model_1.add(Dense(256,activation=('relu'))) \n",
    "        #model_1.add(Dropout(.3))#Adding a dropout layer that will randomly drop 30% of the weights\n",
    "        model_1.add(Dense(128,activation=('relu')))\n",
    "        #model_1.add(Dropout(.2))\n",
    "        model_1.add(Dense(10,activation=('softmax'))) #This is the classification layer\n",
    "        return model_1\n",
    "\n",
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(1 * weight[i])\n",
    "    return weight\n",
    "def scale_model_weights2(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(1 * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "def test_model_mid(X_test, Y_test,  model, comm_round):\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    logits = model.predict(X_test)\n",
    "    loss = cce(Y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
    "    return acc, loss\n",
    "\n",
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    logits = model.predict(X_test)\n",
    "    loss = cce(Y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
    "    print('acc: {:.3%} | loss: {}'.format(acc, loss))\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ae7522-a9f6-46d3-8dad-a246cdfae453",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(False)\n",
    "#On Full dataset\n",
    "SGD_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(y_train)).batch(32)\n",
    "smlp_SGD = SimpleMLP()\n",
    "SGD_model = smlp_SGD.build(3072, 10) \n",
    "lr = 0.01 \n",
    "loss='categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = SGD(learning_rate=lr, \n",
    "                decay=lr , \n",
    "                momentum=0.9\n",
    "               ) \n",
    "SGD_model.compile(loss=loss, \n",
    "              optimizer=optimizer, \n",
    "              metrics=metrics)\n",
    "\n",
    "# fit the SGD training data to model\n",
    "_ = SGD_model.fit(SGD_dataset, epochs=20, verbose=1)\n",
    "\n",
    "#test the SGD global model and print out metrics\n",
    "for(X_test, Y_test) in test_batched:\n",
    "        SGD_acc, SGD_loss = test_model(X_test, Y_test, SGD_model, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ea8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "for(X_test, Y_test) in test_batched:\n",
    "        SGD_acc, SGD_loss = test_model(X_test, Y_test, SGD_model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29996b5-17b1-4152-b52f-7863925cd632",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec42cf8f-17a7-4039-aab8-9799d4e2d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create optimizer\n",
    "comms_round = 10\n",
    "lr = 0.01 \n",
    "loss='categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = SGD(learning_rate=lr, \n",
    "                decay=lr / comms_round, \n",
    "                momentum=0.9\n",
    "               ) \n",
    "smlp_global = SimpleMLP()\n",
    "\n",
    "global_model = smlp_global.build(3072, 10)\n",
    "global_model.compile(loss=loss, \n",
    "              optimizer=optimizer, \n",
    "              metrics=metrics)\n",
    "global_model.build(input_shape=(None,32,32,3))\n",
    "\n",
    "average_weights=global_model.get_weights()\n",
    "print(average_weights)\n",
    "average_weights = np.array(average_weights)\n",
    "\n",
    "average_weights.fill(0)\n",
    "ranking_acc_comp=list() \n",
    "\n",
    "for comm_round in range(comms_round):\n",
    "    average_weights = np.array(average_weights)\n",
    "    average_weights.fill(0)\n",
    "    scaled_local_weight_list = list()\n",
    "    client_names= list(clients_batched.keys())\n",
    "    ranking_acc=list()\n",
    "    ranking_acc.clear()\n",
    "    for client in client_names:\n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build(3072, 10)\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=metrics)\n",
    "        local_model.build(input_shape=(None,32,32,3))\n",
    "        local_model.set_weights(global_model.get_weights())\n",
    "        local_model.fit(clients_batched[client], epochs=2, verbose=1)\n",
    "        weights=local_model.get_weights();\n",
    "        weights = np.array(weights)\n",
    "        average_weights=np.add(average_weights,weights)\n",
    "        \n",
    "        for(X_test, Y_test) in test_batched:\n",
    "         local_acc, local_loss = test_model(X_test, Y_test, local_model, comm_round)\n",
    "        ranking_acc.append(local_acc)\n",
    " \n",
    "        K.clear_session()\n",
    "    average_weights=average_weights/10\n",
    "    average_weights = average_weights.tolist()\n",
    "    \n",
    "    ranking_acc.sort(reverse=True)\n",
    "    temp=0\n",
    "    avg=0\n",
    "    for i in range(comm_round+1):\n",
    "      avg+=ranking_acc[i]\n",
    "    avg=avg/(comm_round+1)\n",
    "    ranking_acc_comp.append(avg)\n",
    "\n",
    "\n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "\n",
    "    #test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d278b1-8d33-4e19-825a-17a794ecba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ranking_acc_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e490abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ranking_acc_comp)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd288a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#commence global training loop\n",
    "for comm_round in range(comms_round):\n",
    "            \n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(clients_batched.keys())\n",
    "    \n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build(784, 10)\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=metrics)\n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #fit local model with client's data\n",
    "        local_model.fit(clients_batched[client], epochs=2, verbose=0)\n",
    " \n",
    "        \n",
    "        #scale the model weights and add to list\n",
    "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "\n",
    "    #test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)\n",
    "        SGD_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(y_train)).batch(320)\n",
    "\n",
    "#gpu sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cd5c73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
