{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.19 (you have 1.4.17). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm import tqdm_notebook\n",
    "%matplotlib inline \n",
    "# from google.colab.patches import cv2_imshow\n",
    "from IPython.display import HTML #imports to play videos\n",
    "from base64 import b64encode \n",
    "import cv2 as cv\n",
    "#from skimage.measure import compare_ssim\n",
    "from torchvision.datasets import ImageFolder\n",
    "import glob\n",
    "import time\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1, extract_face\n",
    "from tqdm import tqdm\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import math\n",
    "import pickle\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "import cv2\n",
    "import skimage.measure\n",
    "import albumentations as A\n",
    "from tqdm.notebook import tqdm \n",
    "#from albumentations.pytorch import ToTensor \n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models.video import mc3_18, r2plus1d_18\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "import sklearn\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "DATA_FOLDER = os.getcwd()\n",
    "\n",
    "BASE_PATH = os.getcwd()\n",
    "\n",
    "ORI_CROPS = r'D:\\FF\\crops\\original_sequences'\n",
    "MANIP_CROPS = r'D:\\FF\\crops\\manipulated_sequences'\n",
    "CROPS_FOLDER = r'D:\\FF\\crops'\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json\n",
    "def get_videos_from_folder(folder_path):\n",
    "    # List all files in the specified folder\n",
    "    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    if 'metadata.json' in files:\n",
    "        files.remove('metadata.json')\n",
    "    full_paths = [os.path.join(folder_path, f) for f in files]\n",
    "    return full_paths\n",
    "\n",
    "def get_videos_basenames_from_folder(folder_path):\n",
    "    # List all files in the specified folder\n",
    "    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    if 'metadata.json' in files:\n",
    "        files.remove('metadata.json')\n",
    "    return files\n",
    "\n",
    "def get_original_with_fakes(root_dir):\n",
    "    pairs = []\n",
    "    for json_path in glob(os.path.join(root_dir, \"*/metadata.json\")):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "        for k, v in metadata.items():\n",
    "            original = v.get(\"original\", None)\n",
    "            if v[\"label\"] == \"FAKE\":\n",
    "                pairs.append((original[:-4], k[:-4] ))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "#metadata['aaqaifqrwn.mp4']['label']\n",
    "\n",
    "def sample_weights(metadata):\n",
    "    video_fake_real_labels = metadata.iloc[0]\n",
    "    class_counts = video_fake_real_labels.value_counts()\n",
    "    class_weights = 1 / class_counts\n",
    "    return [1/class_counts[i] for i in video_fake_real_labels] # for undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:34: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n",
      "c:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:79: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n",
      "c:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:132: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "frames_per_video = 32\n",
    "mtcnn = MTCNN(margin=20, select_largest=False, factor=0.5, device=device, post_process=True) # post_process=False if want human readable image\n",
    "img_size = 224 # 256\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "class ImageTransform:\n",
    "    def __init__(self, size, mean, std):\n",
    "        self.data_transform = transforms.Compose([\n",
    "                transforms.Resize((size, size), interpolation=Image.BILINEAR),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.data_transform(img)\n",
    "    \n",
    "transformer = ImageTransform(img_size, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(root=CROPS_FOLDER, transform=transformer) \n",
    "train_loader = DataLoader(train_dataset, shuffle=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'manipulated_sequences': 0, 'original_sequences': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained_weights_path = 'noisy-student-efficientnet-b4.pth' # noisy student\n",
    "#model = EfficientNet.from_name('efficientnet-b4')\n",
    "#model = EfficientNet.from_name('efficientnet-b1')\n",
    "#model.load_state_dict(torch.load(pretrained_weights_path, map_location=torch.device(device)))\n",
    "#model._fc = nn.Linear(in_features=model._fc.in_features, out_features=1)\n",
    "# Load the pre-trained EfficientNetV2B0 model (using EfficientNet)\n",
    "env2b0_model = models.efficientnet_b0(weights='DEFAULT')\n",
    "# Remove the classification head (last layer)\n",
    "env2b0_model.classifier = nn.Identity()  # or you can use model.classifier = None\n",
    "\n",
    "# Freeze all layers in the pre-trained model\n",
    "for param in env2b0_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Custom top model\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.pretrained = pretrained_model\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dense1 = nn.Linear(1280, 512)  # Adjust for your pretrained model's output\n",
    "        self.batch_norm = nn.BatchNorm1d(512)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.dense2 = nn.Linear(512, 1)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pretrained(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        x = torch.sigmoid(x).squeeze()      # Apply sigmoid activation\n",
    "        return x\n",
    "\n",
    "# Create the final model\n",
    "model = CustomModel(env2b0_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained.features.0.0.weight: requires_grad=False\n",
      "pretrained.features.0.1.weight: requires_grad=False\n",
      "pretrained.features.0.1.bias: requires_grad=False\n",
      "pretrained.features.1.0.block.0.0.weight: requires_grad=False\n",
      "pretrained.features.1.0.block.0.1.weight: requires_grad=False\n",
      "pretrained.features.1.0.block.0.1.bias: requires_grad=False\n",
      "pretrained.features.1.0.block.1.fc1.weight: requires_grad=False\n",
      "pretrained.features.1.0.block.1.fc1.bias: requires_grad=False\n",
      "pretrained.features.1.0.block.1.fc2.weight: requires_grad=False\n",
      "pretrained.features.1.0.block.1.fc2.bias: requires_grad=False\n",
      "pretrained.features.1.0.block.2.0.weight: requires_grad=False\n",
      "pretrained.features.1.0.block.2.1.weight: requires_grad=False\n",
      "pretrained.features.1.0.block.2.1.bias: requires_grad=False\n",
      "pretrained.features.2.0.block.0.0.weight: requires_grad=False\n",
      "pretrained.features.2.0.block.0.1.weight: requires_grad=False\n",
      "pretrained.features.2.0.block.0.1.bias: requires_grad=False\n",
      "pretrained.features.2.0.block.1.0.weight: requires_grad=False\n",
      "pretrained.features.2.0.block.1.1.weight: requires_grad=False\n",
      "pretrained.features.2.0.block.1.1.bias: requires_grad=False\n",
      "pretrained.features.2.0.block.2.fc1.weight: requires_grad=False\n",
      "pretrained.features.2.0.block.2.fc1.bias: requires_grad=False\n",
      "pretrained.features.2.0.block.2.fc2.weight: requires_grad=False\n",
      "pretrained.features.2.0.block.2.fc2.bias: requires_grad=False\n",
      "pretrained.features.2.0.block.3.0.weight: requires_grad=False\n",
      "pretrained.features.2.0.block.3.1.weight: requires_grad=False\n",
      "pretrained.features.2.0.block.3.1.bias: requires_grad=False\n",
      "pretrained.features.2.1.block.0.0.weight: requires_grad=False\n",
      "pretrained.features.2.1.block.0.1.weight: requires_grad=False\n",
      "pretrained.features.2.1.block.0.1.bias: requires_grad=False\n",
      "pretrained.features.2.1.block.1.0.weight: requires_grad=False\n",
      "pretrained.features.2.1.block.1.1.weight: requires_grad=False\n",
      "pretrained.features.2.1.block.1.1.bias: requires_grad=False\n",
      "pretrained.features.2.1.block.2.fc1.weight: requires_grad=False\n",
      "pretrained.features.2.1.block.2.fc1.bias: requires_grad=False\n",
      "pretrained.features.2.1.block.2.fc2.weight: requires_grad=False\n",
      "pretrained.features.2.1.block.2.fc2.bias: requires_grad=False\n",
      "pretrained.features.2.1.block.3.0.weight: requires_grad=False\n",
      "pretrained.features.2.1.block.3.1.weight: requires_grad=False\n",
      "pretrained.features.2.1.block.3.1.bias: requires_grad=False\n",
      "pretrained.features.3.0.block.0.0.weight: requires_grad=False\n",
      "pretrained.features.3.0.block.0.1.weight: requires_grad=False\n",
      "pretrained.features.3.0.block.0.1.bias: requires_grad=False\n",
      "pretrained.features.3.0.block.1.0.weight: requires_grad=False\n",
      "pretrained.features.3.0.block.1.1.weight: requires_grad=False\n",
      "pretrained.features.3.0.block.1.1.bias: requires_grad=False\n",
      "pretrained.features.3.0.block.2.fc1.weight: requires_grad=False\n",
      "pretrained.features.3.0.block.2.fc1.bias: requires_grad=False\n",
      "pretrained.features.3.0.block.2.fc2.weight: requires_grad=False\n",
      "pretrained.features.3.0.block.2.fc2.bias: requires_grad=False\n",
      "pretrained.features.3.0.block.3.0.weight: requires_grad=False\n",
      "pretrained.features.3.0.block.3.1.weight: requires_grad=False\n",
      "pretrained.features.3.0.block.3.1.bias: requires_grad=False\n",
      "pretrained.features.3.1.block.0.0.weight: requires_grad=False\n",
      "pretrained.features.3.1.block.0.1.weight: requires_grad=False\n",
      "pretrained.features.3.1.block.0.1.bias: requires_grad=False\n",
      "pretrained.features.3.1.block.1.0.weight: requires_grad=False\n",
      "pretrained.features.3.1.block.1.1.weight: requires_grad=False\n",
      "pretrained.features.3.1.block.1.1.bias: requires_grad=False\n",
      "pretrained.features.3.1.block.2.fc1.weight: requires_grad=False\n",
      "pretrained.features.3.1.block.2.fc1.bias: requires_grad=False\n",
      "pretrained.features.3.1.block.2.fc2.weight: requires_grad=False\n",
      "pretrained.features.3.1.block.2.fc2.bias: requires_grad=False\n",
      "pretrained.features.3.1.block.3.0.weight: requires_grad=False\n",
      "pretrained.features.3.1.block.3.1.weight: requires_grad=False\n",
      "pretrained.features.3.1.block.3.1.bias: requires_grad=False\n",
      "pretrained.features.4.0.block.0.0.weight: requires_grad=False\n",
      "pretrained.features.4.0.block.0.1.weight: requires_grad=False\n",
      "pretrained.features.4.0.block.0.1.bias: requires_grad=False\n",
      "pretrained.features.4.0.block.1.0.weight: requires_grad=False\n",
      "pretrained.features.4.0.block.1.1.weight: requires_grad=False\n",
      "pretrained.features.4.0.block.1.1.bias: requires_grad=False\n",
      "pretrained.features.4.0.block.2.fc1.weight: requires_grad=False\n",
      "pretrained.features.4.0.block.2.fc1.bias: requires_grad=False\n",
      "pretrained.features.4.0.block.2.fc2.weight: requires_grad=False\n",
      "pretrained.features.4.0.block.2.fc2.bias: requires_grad=False\n",
      "pretrained.features.4.0.block.3.0.weight: requires_grad=False\n",
      "pretrained.features.4.0.block.3.1.weight: requires_grad=False\n",
      "pretrained.features.4.0.block.3.1.bias: requires_grad=False\n",
      "pretrained.features.4.1.block.0.0.weight: requires_grad=False\n",
      "pretrained.features.4.1.block.0.1.weight: requires_grad=False\n",
      "pretrained.features.4.1.block.0.1.bias: requires_grad=False\n",
      "pretrained.features.4.1.block.1.0.weight: requires_grad=False\n",
      "pretrained.features.4.1.block.1.1.weight: requires_grad=False\n",
      "pretrained.features.4.1.block.1.1.bias: requires_grad=False\n",
      "pretrained.features.4.1.block.2.fc1.weight: requires_grad=False\n",
      "pretrained.features.4.1.block.2.fc1.bias: requires_grad=False\n",
      "pretrained.features.4.1.block.2.fc2.weight: requires_grad=False\n",
      "pretrained.features.4.1.block.2.fc2.bias: requires_grad=False\n",
      "pretrained.features.4.1.block.3.0.weight: requires_grad=False\n",
      "pretrained.features.4.1.block.3.1.weight: requires_grad=False\n",
      "pretrained.features.4.1.block.3.1.bias: requires_grad=False\n",
      "pretrained.features.4.2.block.0.0.weight: requires_grad=False\n",
      "pretrained.features.4.2.block.0.1.weight: requires_grad=False\n",
      "pretrained.features.4.2.block.0.1.bias: requires_grad=False\n",
      "pretrained.features.4.2.block.1.0.weight: requires_grad=False\n",
      "pretrained.features.4.2.block.1.1.weight: requires_grad=False\n",
      "pretrained.features.4.2.block.1.1.bias: requires_grad=False\n",
      "pretrained.features.4.2.block.2.fc1.weight: requires_grad=False\n",
      "pretrained.features.4.2.block.2.fc1.bias: requires_grad=False\n",
      "pretrained.features.4.2.block.2.fc2.weight: requires_grad=False\n",
      "pretrained.features.4.2.block.2.fc2.bias: requires_grad=False\n",
      "pretrained.features.4.2.block.3.0.weight: requires_grad=False\n",
      "pretrained.features.4.2.block.3.1.weight: requires_grad=False\n",
      "pretrained.features.4.2.block.3.1.bias: requires_grad=False\n",
      "pretrained.features.5.0.block.0.0.weight: requires_grad=False\n",
      "pretrained.features.5.0.block.0.1.weight: requires_grad=False\n",
      "pretrained.features.5.0.block.0.1.bias: requires_grad=False\n",
      "pretrained.features.5.0.block.1.0.weight: requires_grad=False\n",
      "pretrained.features.5.0.block.1.1.weight: requires_grad=False\n",
      "pretrained.features.5.0.block.1.1.bias: requires_grad=False\n",
      "pretrained.features.5.0.block.2.fc1.weight: requires_grad=False\n",
      "pretrained.features.5.0.block.2.fc1.bias: requires_grad=False\n",
      "pretrained.features.5.0.block.2.fc2.weight: requires_grad=False\n",
      "pretrained.features.5.0.block.2.fc2.bias: requires_grad=False\n",
      "pretrained.features.5.0.block.3.0.weight: requires_grad=False\n",
      "pretrained.features.5.0.block.3.1.weight: requires_grad=False\n",
      "pretrained.features.5.0.block.3.1.bias: requires_grad=False\n",
      "pretrained.features.5.1.block.0.0.weight: requires_grad=False\n",
      "pretrained.features.5.1.block.0.1.weight: requires_grad=False\n",
      "pretrained.features.5.1.block.0.1.bias: requires_grad=False\n",
      "pretrained.features.5.1.block.1.0.weight: requires_grad=False\n",
      "pretrained.features.5.1.block.1.1.weight: requires_grad=False\n",
      "pretrained.features.5.1.block.1.1.bias: requires_grad=False\n",
      "pretrained.features.5.1.block.2.fc1.weight: requires_grad=False\n",
      "pretrained.features.5.1.block.2.fc1.bias: requires_grad=False\n",
      "pretrained.features.5.1.block.2.fc2.weight: requires_grad=False\n",
      "pretrained.features.5.1.block.2.fc2.bias: requires_grad=False\n",
      "pretrained.features.5.1.block.3.0.weight: requires_grad=False\n",
      "pretrained.features.5.1.block.3.1.weight: requires_grad=False\n",
      "pretrained.features.5.1.block.3.1.bias: requires_grad=False\n",
      "pretrained.features.5.2.block.0.0.weight: requires_grad=False\n",
      "pretrained.features.5.2.block.0.1.weight: requires_grad=False\n",
      "pretrained.features.5.2.block.0.1.bias: requires_grad=False\n",
      "pretrained.features.5.2.block.1.0.weight: requires_grad=False\n",
      "pretrained.features.5.2.block.1.1.weight: requires_grad=False\n",
      "pretrained.features.5.2.block.1.1.bias: requires_grad=False\n",
      "pretrained.features.5.2.block.2.fc1.weight: requires_grad=False\n",
      "pretrained.features.5.2.block.2.fc1.bias: requires_grad=False\n",
      "pretrained.features.5.2.block.2.fc2.weight: requires_grad=False\n",
      "pretrained.features.5.2.block.2.fc2.bias: requires_grad=False\n",
      "pretrained.features.5.2.block.3.0.weight: requires_grad=False\n",
      "pretrained.features.5.2.block.3.1.weight: requires_grad=False\n",
      "pretrained.features.5.2.block.3.1.bias: requires_grad=False\n",
      "pretrained.features.6.0.block.0.0.weight: requires_grad=False\n",
      "pretrained.features.6.0.block.0.1.weight: requires_grad=False\n",
      "pretrained.features.6.0.block.0.1.bias: requires_grad=False\n",
      "pretrained.features.6.0.block.1.0.weight: requires_grad=False\n",
      "pretrained.features.6.0.block.1.1.weight: requires_grad=False\n",
      "pretrained.features.6.0.block.1.1.bias: requires_grad=False\n",
      "pretrained.features.6.0.block.2.fc1.weight: requires_grad=False\n",
      "pretrained.features.6.0.block.2.fc1.bias: requires_grad=False\n",
      "pretrained.features.6.0.block.2.fc2.weight: requires_grad=False\n",
      "pretrained.features.6.0.block.2.fc2.bias: requires_grad=False\n",
      "pretrained.features.6.0.block.3.0.weight: requires_grad=False\n",
      "pretrained.features.6.0.block.3.1.weight: requires_grad=False\n",
      "pretrained.features.6.0.block.3.1.bias: requires_grad=False\n",
      "pretrained.features.6.1.block.0.0.weight: requires_grad=False\n",
      "pretrained.features.6.1.block.0.1.weight: requires_grad=False\n",
      "pretrained.features.6.1.block.0.1.bias: requires_grad=False\n",
      "pretrained.features.6.1.block.1.0.weight: requires_grad=False\n",
      "pretrained.features.6.1.block.1.1.weight: requires_grad=False\n",
      "pretrained.features.6.1.block.1.1.bias: requires_grad=False\n",
      "pretrained.features.6.1.block.2.fc1.weight: requires_grad=False\n",
      "pretrained.features.6.1.block.2.fc1.bias: requires_grad=False\n",
      "pretrained.features.6.1.block.2.fc2.weight: requires_grad=False\n",
      "pretrained.features.6.1.block.2.fc2.bias: requires_grad=False\n",
      "pretrained.features.6.1.block.3.0.weight: requires_grad=False\n",
      "pretrained.features.6.1.block.3.1.weight: requires_grad=False\n",
      "pretrained.features.6.1.block.3.1.bias: requires_grad=False\n",
      "pretrained.features.6.2.block.0.0.weight: requires_grad=False\n",
      "pretrained.features.6.2.block.0.1.weight: requires_grad=False\n",
      "pretrained.features.6.2.block.0.1.bias: requires_grad=False\n",
      "pretrained.features.6.2.block.1.0.weight: requires_grad=False\n",
      "pretrained.features.6.2.block.1.1.weight: requires_grad=False\n",
      "pretrained.features.6.2.block.1.1.bias: requires_grad=False\n",
      "pretrained.features.6.2.block.2.fc1.weight: requires_grad=False\n",
      "pretrained.features.6.2.block.2.fc1.bias: requires_grad=False\n",
      "pretrained.features.6.2.block.2.fc2.weight: requires_grad=False\n",
      "pretrained.features.6.2.block.2.fc2.bias: requires_grad=False\n",
      "pretrained.features.6.2.block.3.0.weight: requires_grad=False\n",
      "pretrained.features.6.2.block.3.1.weight: requires_grad=False\n",
      "pretrained.features.6.2.block.3.1.bias: requires_grad=False\n",
      "pretrained.features.6.3.block.0.0.weight: requires_grad=False\n",
      "pretrained.features.6.3.block.0.1.weight: requires_grad=False\n",
      "pretrained.features.6.3.block.0.1.bias: requires_grad=False\n",
      "pretrained.features.6.3.block.1.0.weight: requires_grad=False\n",
      "pretrained.features.6.3.block.1.1.weight: requires_grad=False\n",
      "pretrained.features.6.3.block.1.1.bias: requires_grad=False\n",
      "pretrained.features.6.3.block.2.fc1.weight: requires_grad=False\n",
      "pretrained.features.6.3.block.2.fc1.bias: requires_grad=False\n",
      "pretrained.features.6.3.block.2.fc2.weight: requires_grad=False\n",
      "pretrained.features.6.3.block.2.fc2.bias: requires_grad=False\n",
      "pretrained.features.6.3.block.3.0.weight: requires_grad=False\n",
      "pretrained.features.6.3.block.3.1.weight: requires_grad=False\n",
      "pretrained.features.6.3.block.3.1.bias: requires_grad=False\n",
      "pretrained.features.7.0.block.0.0.weight: requires_grad=False\n",
      "pretrained.features.7.0.block.0.1.weight: requires_grad=False\n",
      "pretrained.features.7.0.block.0.1.bias: requires_grad=False\n",
      "pretrained.features.7.0.block.1.0.weight: requires_grad=False\n",
      "pretrained.features.7.0.block.1.1.weight: requires_grad=False\n",
      "pretrained.features.7.0.block.1.1.bias: requires_grad=False\n",
      "pretrained.features.7.0.block.2.fc1.weight: requires_grad=False\n",
      "pretrained.features.7.0.block.2.fc1.bias: requires_grad=False\n",
      "pretrained.features.7.0.block.2.fc2.weight: requires_grad=False\n",
      "pretrained.features.7.0.block.2.fc2.bias: requires_grad=False\n",
      "pretrained.features.7.0.block.3.0.weight: requires_grad=False\n",
      "pretrained.features.7.0.block.3.1.weight: requires_grad=False\n",
      "pretrained.features.7.0.block.3.1.bias: requires_grad=False\n",
      "pretrained.features.8.0.weight: requires_grad=False\n",
      "pretrained.features.8.1.weight: requires_grad=False\n",
      "pretrained.features.8.1.bias: requires_grad=False\n",
      "dense1.weight: requires_grad=True\n",
      "dense1.bias: requires_grad=True\n",
      "batch_norm.weight: requires_grad=True\n",
      "batch_norm.bias: requires_grad=True\n",
      "dense2.weight: requires_grad=True\n",
      "dense2.bias: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=48, pin_memory=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "\n",
    "train_loss_history = []\n",
    "train_accuracy_history = []\n",
    "val_loss_history = []\n",
    "val_accuracy_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=5, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = float('inf')  # Initialize to positive infinity\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def early_stop(self, val_loss, model):\n",
    "        if val_loss < self.best_score:\n",
    "            self.best_score = val_loss  # Update best validation loss\n",
    "            self.counter = 0  # Reset patience counter\n",
    "            torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "        else:\n",
    "            self.counter += 1  # Increment patience counter\n",
    "           \n",
    "        if self.counter >= self.patience:\n",
    "            if self.verbose:\n",
    "                print(\"Early stopping...\")\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Train Loss: 0.01166, Train Accuracy: 0.50042\n",
      "Epoch [1/100], Train Loss: 0.01166, Train Accuracy: 0.49811\n",
      "Epoch [2/100], Train Loss: 0.01166, Train Accuracy: 0.49954\n",
      "Epoch [3/100], Train Loss: 0.01164, Train Accuracy: 0.50268\n",
      "Epoch [4/100], Train Loss: 0.01166, Train Accuracy: 0.50062\n",
      "Epoch [5/100], Train Loss: 0.01165, Train Accuracy: 0.50115\n",
      "Epoch [6/100], Train Loss: 0.01168, Train Accuracy: 0.49802\n",
      "Epoch [7/100], Train Loss: 0.01166, Train Accuracy: 0.50208\n",
      "Epoch [8/100], Train Loss: 0.01166, Train Accuracy: 0.49855\n",
      "Epoch [9/100], Train Loss: 0.01165, Train Accuracy: 0.50129\n",
      "Epoch [10/100], Train Loss: 0.01165, Train Accuracy: 0.49882\n",
      "Epoch [11/100], Train Loss: 0.01165, Train Accuracy: 0.50093\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "model = model.to(device)\n",
    "earlystopper = EarlyStopper()\n",
    "# mtcnn unable to detect gan/diffusion faces\n",
    "#data = data.to(device)  # Move data to GPU if available\n",
    "# half precision\n",
    "# A6000 at desktop\n",
    "for epoch in range(epochs): \n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for data, labels in train_loader:\n",
    "        data = data.squeeze(0)\n",
    "        labels = labels.squeeze(0)\n",
    "        labels = labels.to(device).float()\n",
    "        optimizer.zero_grad()  # clear previous gradients\n",
    "\n",
    "        data = data.to(device).float()\n",
    "        outputs = model(data)\n",
    "        losses = loss_fn(outputs, labels)\n",
    "        running_loss += losses.item()  # accumulate loss\n",
    "        predicted = (outputs >= 0.5).float() # calculate if label is 0 or 1\n",
    "        losses.backward()\n",
    "        correct += (predicted == labels).sum().item() \n",
    "        total += labels.size(0)\n",
    "\n",
    "    average_train_loss = running_loss / total\n",
    "    average_train_accuracy = correct / total\n",
    "    train_loss_history.append(average_train_loss)\n",
    "    train_accuracy_history.append(average_train_accuracy)\n",
    "    print(f\"Epoch [{epoch}/{epochs}], Train Loss: {average_train_loss:.5f}, Train Accuracy: {average_train_accuracy:.5f}\")\n",
    "\n",
    "    \n",
    "torch.save(model.state_dict(), 'model_ff.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5360, device='cuda:0', grad_fn=<MeanBackward1>) tensor([1], device='cuda:0')\n",
      "tensor(0.5374, device='cuda:0', grad_fn=<MeanBackward1>) tensor([1], device='cuda:0')\n",
      "tensor(0.5356, device='cuda:0', grad_fn=<MeanBackward1>) tensor([0], device='cuda:0')\n",
      "tensor(0.5421, device='cuda:0', grad_fn=<MeanBackward1>) tensor([0], device='cuda:0')\n",
      "tensor(0.8123, device='cuda:0', grad_fn=<MeanBackward1>) tensor([0], device='cuda:0')\n",
      "tensor(0.6681, device='cuda:0', grad_fn=<MeanBackward1>) tensor([1], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[6], line 26\u001b[0m, in \u001b[0;36mDeepfakeDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     _frame \u001b[38;5;241m=\u001b[39m frame[np\u001b[38;5;241m.\u001b[39mnewaxis, :, :, :]\n\u001b[1;32m---> 26\u001b[0m     boxes, probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlandmarks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(boxes[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     28\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(boxes[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:313\u001b[0m, in \u001b[0;36mMTCNN.detect\u001b[1;34m(self, img, landmarks)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03mThis method is used by the forward method and is also useful for face detection tasks\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m>>> img_draw.save('annotated_faces.png')\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 313\u001b[0m     batch_boxes, batch_points \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_face\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_face_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthresholds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m boxes, probs, points \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m box, point \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_boxes, batch_points):\n",
      "File \u001b[1;32mc:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py:93\u001b[0m, in \u001b[0;36mdetect_face\u001b[1;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[0;32m     89\u001b[0m boxes, image_inds \u001b[38;5;241m=\u001b[39m boxes[scale_picks], image_inds[scale_picks]\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# NMS within each image\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m pick \u001b[38;5;241m=\u001b[39m \u001b[43mbatched_nms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_inds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m boxes, image_inds \u001b[38;5;241m=\u001b[39m boxes[pick], image_inds[pick]\n\u001b[0;32m     96\u001b[0m regw \u001b[38;5;241m=\u001b[39m boxes[:, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m boxes[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\ops\\boxes.py:75\u001b[0m, in \u001b[0;36mbatched_nms\u001b[1;34m(boxes, scores, idxs, iou_threshold)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _batched_nms_vanilla(boxes, scores, idxs, iou_threshold)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_batched_nms_coordinate_trick\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\jit\\_trace.py:1441\u001b[0m, in \u001b[0;36m_script_if_tracing.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m R:\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing():\n\u001b[0;32m   1440\u001b[0m         \u001b[38;5;66;03m# Not tracing, don't do anything\u001b[39;00m\n\u001b[1;32m-> 1441\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1443\u001b[0m     compiled_fn: Callable[P, R] \u001b[38;5;241m=\u001b[39m script(wrapper\u001b[38;5;241m.\u001b[39m__original_fn)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1444\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\ops\\boxes.py:91\u001b[0m, in \u001b[0;36m_batched_nms_coordinate_trick\u001b[1;34m(boxes, scores, idxs, iou_threshold)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m boxes\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m0\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mboxes\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 91\u001b[0m max_coordinate \u001b[38;5;241m=\u001b[39m \u001b[43mboxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m offsets \u001b[38;5;241m=\u001b[39m idxs\u001b[38;5;241m.\u001b[39mto(boxes) \u001b[38;5;241m*\u001b[39m (max_coordinate \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(boxes))\n\u001b[0;32m     93\u001b[0m boxes_for_nms \u001b[38;5;241m=\u001b[39m boxes \u001b[38;5;241m+\u001b[39m offsets[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for data, label in train_loader:\n",
    "        label = label.to(device)\n",
    "        outputs=[]\n",
    "        for face in data:\n",
    "            face = face.to(device)\n",
    "            output = model(face)\n",
    "            outputs.append(output)\n",
    "        answer = torch.mean(torch.stack(outputs), dim=0)\n",
    "        print(answer, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained_weights_path = 'noisy-student-efficientnet-b4.pth' # noisy student\n",
    "#model = EfficientNet.from_name('efficientnet-b4')\n",
    "#model.load_state_dict(torch.load(pretrained_weights_path, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
