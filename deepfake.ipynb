{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.17). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm import tqdm_notebook\n",
    "%matplotlib inline \n",
    "# from google.colab.patches import cv2_imshow\n",
    "from IPython.display import HTML #imports to play videos\n",
    "from base64 import b64encode \n",
    "import cv2 as cv\n",
    "#from skimage.measure import compare_ssim\n",
    "import glob\n",
    "import time\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1, extract_face\n",
    "from tqdm import tqdm\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import math\n",
    "import pickle\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "import skimage.measure\n",
    "import albumentations as A\n",
    "from tqdm.notebook import tqdm \n",
    "#from albumentations.pytorch import ToTensor \n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models.video import mc3_18, r2plus1d_18\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "import sklearn\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "SOURCE_FOLDER = r\"D:\\dfdc\"\n",
    "TRAIN_FOLDER = os.path.join(SOURCE_FOLDER, 'dfdc_videos', \"dfdc_train_part_1\") \n",
    "TEST_FOLDER = os.path.join(SOURCE_FOLDER, 'dfdc_videos', \"dfdc_train_part_0\") \n",
    "\n",
    "TRAIN_FOLDER_CROPS = os.path.join(SOURCE_FOLDER, 'crops', \"dfdc_train_part_1\") \n",
    "TEST_FOLDER_CROPS = os.path.join(SOURCE_FOLDER, 'crops', \"dfdc_train_part_0\") \n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json\n",
    "def get_videos_from_folder(folder_path):\n",
    "    # List all files in the specified folder\n",
    "    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    if 'metadata.json' in files:\n",
    "        files.remove('metadata.json')\n",
    "    full_paths = [os.path.join(folder_path, f) for f in files]\n",
    "    return full_paths\n",
    "\n",
    "def get_videos_basenames_from_folder(folder_path):\n",
    "    # List all files in the specified folder\n",
    "    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    if 'metadata.json' in files:\n",
    "        files.remove('metadata.json')\n",
    "    return files\n",
    "\n",
    "def get_original_with_fakes(root_dir):\n",
    "    pairs = []\n",
    "    for json_path in glob(os.path.join(root_dir, \"*/metadata.json\")):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "        for k, v in metadata.items():\n",
    "            original = v.get(\"original\", None)\n",
    "            if v[\"label\"] == \"FAKE\":\n",
    "                pairs.append((original[:-4], k[:-4] ))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "metadata_val = pd.read_json(TEST_FOLDER+ '/metadata.json')\n",
    "metadata_train = pd.read_json(TRAIN_FOLDER+ '/metadata.json')\n",
    "#metadata['aaqaifqrwn.mp4']['label']\n",
    "\n",
    "def sample_weights(metadata):\n",
    "    video_fake_real_labels = metadata.iloc[0]\n",
    "    class_counts = video_fake_real_labels.value_counts()\n",
    "    class_weights = 1 / class_counts\n",
    "    return [1/class_counts[i] for i in video_fake_real_labels] # for undersampling\n",
    "\n",
    "train_weights_sampler = WeightedRandomSampler(weights=sample_weights(metadata_train), num_samples=len(metadata_train.iloc[0]), replacement=True)\n",
    "val_weights_sampler =WeightedRandomSampler(weights=sample_weights(metadata_val), num_samples=len(metadata_val.iloc[0]), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set will be 00\n",
    "metadata_val = pd.read_json(TEST_FOLDER+ '/metadata.json')\n",
    "metadata_train = pd.read_json(TRAIN_FOLDER+ '/metadata.json')\n",
    "video_fake_real_labels = metadata_train.iloc[0]\n",
    "real_df = video_fake_real_labels[video_fake_real_labels == 'REAL'].keys()\n",
    "fake_df = video_fake_real_labels[video_fake_real_labels == 'REAL'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, metadata, device, mode='train'):\n",
    "        self.device = device\n",
    "        self.folder_path = TRAIN_FOLDER if mode=='train' else TEST_FOLDER\n",
    "        self.folder_path_crops = TRAIN_FOLDER_CROPS if mode=='train' else TEST_FOLDER_CROPS\n",
    "        self.video_fake_real_labels = metadata.iloc[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_fake_real_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.video_fake_real_labels.index[idx]\n",
    "        label = 0 if self.video_fake_real_labels[filename] == 'REAL' else 1\n",
    "        crops_folder_path = os.path.join(self.folder_path_crops, filename[:-4])\n",
    "        crops_list = [os.path.join(crops_folder_path, f) for f in os.listdir(crops_folder_path) if os.path.isfile(os.path.join(crops_folder_path, f))]\n",
    "        # get crops from folder\n",
    "        crop_tensors = []\n",
    "        \n",
    "        for crop_file in crops_list:\n",
    "            loaded_tensor = torch.load(crop_file)\n",
    "            crop_tensors.append(loaded_tensor)\n",
    "        \n",
    "        crop_tensors = torch.stack(crop_tensors, dim=0) # stack tensors. this is a \"batch\"\n",
    "        labels = torch.tensor([label for i in range(len(crops_list))]) # label for each frame is the same as video label\n",
    "\n",
    "        return crop_tensors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "#pretrained_weights_path = 'noisy-student-efficientnet-b4.pth' # noisy student\n",
    "#model = EfficientNet.from_name('efficientnet-b4')\n",
    "#model = EfficientNet.from_name('efficientnet-b1')\n",
    "#model.load_state_dict(torch.load(pretrained_weights_path, map_location=torch.device(device)))\n",
    "#model._fc = nn.Linear(in_features=model._fc.in_features, out_features=1)\n",
    "class EffnetTest(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EffnetTest, self).__init__()\n",
    "        self.model = EfficientNet.from_pretrained(f\"efficientnet-b0\")\n",
    "        self.model._fc = nn.Linear(in_features=self.model._fc.in_features, out_features=1)\n",
    "        #self.model._norm_layer = nn.GroupNorm(num_groups=32, num_channels=3)\n",
    "        #for i in range(4):\n",
    "        #    for param in self.model._blocks[i].parameters():\n",
    "        #        param.requires_grad = False\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze the last layer\n",
    "        for param in self.model._fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return torch.sigmoid(x).squeeze()\n",
    "model = EffnetTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model._conv_stem.weight False\n",
      "model._bn0.weight False\n",
      "model._bn0.bias False\n",
      "model._blocks.0._depthwise_conv.weight False\n",
      "model._blocks.0._bn1.weight False\n",
      "model._blocks.0._bn1.bias False\n",
      "model._blocks.0._se_reduce.weight False\n",
      "model._blocks.0._se_reduce.bias False\n",
      "model._blocks.0._se_expand.weight False\n",
      "model._blocks.0._se_expand.bias False\n",
      "model._blocks.0._project_conv.weight False\n",
      "model._blocks.0._bn2.weight False\n",
      "model._blocks.0._bn2.bias False\n",
      "model._blocks.1._expand_conv.weight False\n",
      "model._blocks.1._bn0.weight False\n",
      "model._blocks.1._bn0.bias False\n",
      "model._blocks.1._depthwise_conv.weight False\n",
      "model._blocks.1._bn1.weight False\n",
      "model._blocks.1._bn1.bias False\n",
      "model._blocks.1._se_reduce.weight False\n",
      "model._blocks.1._se_reduce.bias False\n",
      "model._blocks.1._se_expand.weight False\n",
      "model._blocks.1._se_expand.bias False\n",
      "model._blocks.1._project_conv.weight False\n",
      "model._blocks.1._bn2.weight False\n",
      "model._blocks.1._bn2.bias False\n",
      "model._blocks.2._expand_conv.weight False\n",
      "model._blocks.2._bn0.weight False\n",
      "model._blocks.2._bn0.bias False\n",
      "model._blocks.2._depthwise_conv.weight False\n",
      "model._blocks.2._bn1.weight False\n",
      "model._blocks.2._bn1.bias False\n",
      "model._blocks.2._se_reduce.weight False\n",
      "model._blocks.2._se_reduce.bias False\n",
      "model._blocks.2._se_expand.weight False\n",
      "model._blocks.2._se_expand.bias False\n",
      "model._blocks.2._project_conv.weight False\n",
      "model._blocks.2._bn2.weight False\n",
      "model._blocks.2._bn2.bias False\n",
      "model._blocks.3._expand_conv.weight False\n",
      "model._blocks.3._bn0.weight False\n",
      "model._blocks.3._bn0.bias False\n",
      "model._blocks.3._depthwise_conv.weight False\n",
      "model._blocks.3._bn1.weight False\n",
      "model._blocks.3._bn1.bias False\n",
      "model._blocks.3._se_reduce.weight False\n",
      "model._blocks.3._se_reduce.bias False\n",
      "model._blocks.3._se_expand.weight False\n",
      "model._blocks.3._se_expand.bias False\n",
      "model._blocks.3._project_conv.weight False\n",
      "model._blocks.3._bn2.weight False\n",
      "model._blocks.3._bn2.bias False\n",
      "model._blocks.4._expand_conv.weight False\n",
      "model._blocks.4._bn0.weight False\n",
      "model._blocks.4._bn0.bias False\n",
      "model._blocks.4._depthwise_conv.weight False\n",
      "model._blocks.4._bn1.weight False\n",
      "model._blocks.4._bn1.bias False\n",
      "model._blocks.4._se_reduce.weight False\n",
      "model._blocks.4._se_reduce.bias False\n",
      "model._blocks.4._se_expand.weight False\n",
      "model._blocks.4._se_expand.bias False\n",
      "model._blocks.4._project_conv.weight False\n",
      "model._blocks.4._bn2.weight False\n",
      "model._blocks.4._bn2.bias False\n",
      "model._blocks.5._expand_conv.weight False\n",
      "model._blocks.5._bn0.weight False\n",
      "model._blocks.5._bn0.bias False\n",
      "model._blocks.5._depthwise_conv.weight False\n",
      "model._blocks.5._bn1.weight False\n",
      "model._blocks.5._bn1.bias False\n",
      "model._blocks.5._se_reduce.weight False\n",
      "model._blocks.5._se_reduce.bias False\n",
      "model._blocks.5._se_expand.weight False\n",
      "model._blocks.5._se_expand.bias False\n",
      "model._blocks.5._project_conv.weight False\n",
      "model._blocks.5._bn2.weight False\n",
      "model._blocks.5._bn2.bias False\n",
      "model._blocks.6._expand_conv.weight False\n",
      "model._blocks.6._bn0.weight False\n",
      "model._blocks.6._bn0.bias False\n",
      "model._blocks.6._depthwise_conv.weight False\n",
      "model._blocks.6._bn1.weight False\n",
      "model._blocks.6._bn1.bias False\n",
      "model._blocks.6._se_reduce.weight False\n",
      "model._blocks.6._se_reduce.bias False\n",
      "model._blocks.6._se_expand.weight False\n",
      "model._blocks.6._se_expand.bias False\n",
      "model._blocks.6._project_conv.weight False\n",
      "model._blocks.6._bn2.weight False\n",
      "model._blocks.6._bn2.bias False\n",
      "model._blocks.7._expand_conv.weight False\n",
      "model._blocks.7._bn0.weight False\n",
      "model._blocks.7._bn0.bias False\n",
      "model._blocks.7._depthwise_conv.weight False\n",
      "model._blocks.7._bn1.weight False\n",
      "model._blocks.7._bn1.bias False\n",
      "model._blocks.7._se_reduce.weight False\n",
      "model._blocks.7._se_reduce.bias False\n",
      "model._blocks.7._se_expand.weight False\n",
      "model._blocks.7._se_expand.bias False\n",
      "model._blocks.7._project_conv.weight False\n",
      "model._blocks.7._bn2.weight False\n",
      "model._blocks.7._bn2.bias False\n",
      "model._blocks.8._expand_conv.weight False\n",
      "model._blocks.8._bn0.weight False\n",
      "model._blocks.8._bn0.bias False\n",
      "model._blocks.8._depthwise_conv.weight False\n",
      "model._blocks.8._bn1.weight False\n",
      "model._blocks.8._bn1.bias False\n",
      "model._blocks.8._se_reduce.weight False\n",
      "model._blocks.8._se_reduce.bias False\n",
      "model._blocks.8._se_expand.weight False\n",
      "model._blocks.8._se_expand.bias False\n",
      "model._blocks.8._project_conv.weight False\n",
      "model._blocks.8._bn2.weight False\n",
      "model._blocks.8._bn2.bias False\n",
      "model._blocks.9._expand_conv.weight False\n",
      "model._blocks.9._bn0.weight False\n",
      "model._blocks.9._bn0.bias False\n",
      "model._blocks.9._depthwise_conv.weight False\n",
      "model._blocks.9._bn1.weight False\n",
      "model._blocks.9._bn1.bias False\n",
      "model._blocks.9._se_reduce.weight False\n",
      "model._blocks.9._se_reduce.bias False\n",
      "model._blocks.9._se_expand.weight False\n",
      "model._blocks.9._se_expand.bias False\n",
      "model._blocks.9._project_conv.weight False\n",
      "model._blocks.9._bn2.weight False\n",
      "model._blocks.9._bn2.bias False\n",
      "model._blocks.10._expand_conv.weight False\n",
      "model._blocks.10._bn0.weight False\n",
      "model._blocks.10._bn0.bias False\n",
      "model._blocks.10._depthwise_conv.weight False\n",
      "model._blocks.10._bn1.weight False\n",
      "model._blocks.10._bn1.bias False\n",
      "model._blocks.10._se_reduce.weight False\n",
      "model._blocks.10._se_reduce.bias False\n",
      "model._blocks.10._se_expand.weight False\n",
      "model._blocks.10._se_expand.bias False\n",
      "model._blocks.10._project_conv.weight False\n",
      "model._blocks.10._bn2.weight False\n",
      "model._blocks.10._bn2.bias False\n",
      "model._blocks.11._expand_conv.weight False\n",
      "model._blocks.11._bn0.weight False\n",
      "model._blocks.11._bn0.bias False\n",
      "model._blocks.11._depthwise_conv.weight False\n",
      "model._blocks.11._bn1.weight False\n",
      "model._blocks.11._bn1.bias False\n",
      "model._blocks.11._se_reduce.weight False\n",
      "model._blocks.11._se_reduce.bias False\n",
      "model._blocks.11._se_expand.weight False\n",
      "model._blocks.11._se_expand.bias False\n",
      "model._blocks.11._project_conv.weight False\n",
      "model._blocks.11._bn2.weight False\n",
      "model._blocks.11._bn2.bias False\n",
      "model._blocks.12._expand_conv.weight False\n",
      "model._blocks.12._bn0.weight False\n",
      "model._blocks.12._bn0.bias False\n",
      "model._blocks.12._depthwise_conv.weight False\n",
      "model._blocks.12._bn1.weight False\n",
      "model._blocks.12._bn1.bias False\n",
      "model._blocks.12._se_reduce.weight False\n",
      "model._blocks.12._se_reduce.bias False\n",
      "model._blocks.12._se_expand.weight False\n",
      "model._blocks.12._se_expand.bias False\n",
      "model._blocks.12._project_conv.weight False\n",
      "model._blocks.12._bn2.weight False\n",
      "model._blocks.12._bn2.bias False\n",
      "model._blocks.13._expand_conv.weight False\n",
      "model._blocks.13._bn0.weight False\n",
      "model._blocks.13._bn0.bias False\n",
      "model._blocks.13._depthwise_conv.weight False\n",
      "model._blocks.13._bn1.weight False\n",
      "model._blocks.13._bn1.bias False\n",
      "model._blocks.13._se_reduce.weight False\n",
      "model._blocks.13._se_reduce.bias False\n",
      "model._blocks.13._se_expand.weight False\n",
      "model._blocks.13._se_expand.bias False\n",
      "model._blocks.13._project_conv.weight False\n",
      "model._blocks.13._bn2.weight False\n",
      "model._blocks.13._bn2.bias False\n",
      "model._blocks.14._expand_conv.weight False\n",
      "model._blocks.14._bn0.weight False\n",
      "model._blocks.14._bn0.bias False\n",
      "model._blocks.14._depthwise_conv.weight False\n",
      "model._blocks.14._bn1.weight False\n",
      "model._blocks.14._bn1.bias False\n",
      "model._blocks.14._se_reduce.weight False\n",
      "model._blocks.14._se_reduce.bias False\n",
      "model._blocks.14._se_expand.weight False\n",
      "model._blocks.14._se_expand.bias False\n",
      "model._blocks.14._project_conv.weight False\n",
      "model._blocks.14._bn2.weight False\n",
      "model._blocks.14._bn2.bias False\n",
      "model._blocks.15._expand_conv.weight False\n",
      "model._blocks.15._bn0.weight False\n",
      "model._blocks.15._bn0.bias False\n",
      "model._blocks.15._depthwise_conv.weight False\n",
      "model._blocks.15._bn1.weight False\n",
      "model._blocks.15._bn1.bias False\n",
      "model._blocks.15._se_reduce.weight False\n",
      "model._blocks.15._se_reduce.bias False\n",
      "model._blocks.15._se_expand.weight False\n",
      "model._blocks.15._se_expand.bias False\n",
      "model._blocks.15._project_conv.weight False\n",
      "model._blocks.15._bn2.weight False\n",
      "model._blocks.15._bn2.bias False\n",
      "model._conv_head.weight False\n",
      "model._bn1.weight False\n",
      "model._bn1.bias False\n",
      "model._fc.weight True\n",
      "model._fc.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=5, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = float('inf')  # Initialize to positive infinity\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def early_stop(self, val_loss, model):\n",
    "        if val_loss < self.best_score:\n",
    "            self.best_score = val_loss  # Update best validation loss\n",
    "            self.counter = 0  # Reset patience counter\n",
    "            torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "        else:\n",
    "            self.counter += 1  # Increment patience counter\n",
    "           \n",
    "        if self.counter >= self.patience:\n",
    "            if self.verbose:\n",
    "                print(\"Early stopping...\")\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DeepfakeDataset(metadata_train, device, mode='train')\n",
    "train_loader = DataLoader(train_dataset, shuffle=False, sampler=train_weights_sampler, batch_size=1, num_workers=0) # process video 1 by 1\n",
    "val_dataset = DeepfakeDataset(metadata_val, device, mode='val')\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, sampler=val_weights_sampler, batch_size=1, num_workers=0) # process video 1 by 1\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_=48, pin_memory=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "train_loss_history = []\n",
    "train_accuracy_history = []\n",
    "val_loss_history = []\n",
    "val_accuracy_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sheryl\\AppData\\Local\\Temp\\ipykernel_22736\\3301423534.py:20: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Train Loss: 0.0228, Train Accuracy: 0.4949, Validation Loss: 0.0219, Validation Accuracy: 0.5217\n",
      "Epoch [1/100], Train Loss: 0.0227, Train Accuracy: 0.4959, Validation Loss: 0.0219, Validation Accuracy: 0.5400\n",
      "Epoch [2/100], Train Loss: 0.0227, Train Accuracy: 0.4980, Validation Loss: 0.0223, Validation Accuracy: 0.5283\n",
      "Epoch [3/100], Train Loss: 0.0227, Train Accuracy: 0.4976, Validation Loss: 0.1132, Validation Accuracy: 0.5366\n",
      "Epoch [4/100], Train Loss: 0.0227, Train Accuracy: 0.4992, Validation Loss: 0.0221, Validation Accuracy: 0.5225\n",
      "Epoch [5/100], Train Loss: 0.0227, Train Accuracy: 0.4951, Validation Loss: 0.0226, Validation Accuracy: 0.5165\n",
      "Epoch [6/100], Train Loss: 0.0227, Train Accuracy: 0.5014, Validation Loss: 0.0229, Validation Accuracy: 0.5287\n",
      "Epoch [7/100], Train Loss: 0.0228, Train Accuracy: 0.5018, Validation Loss: 0.0242, Validation Accuracy: 0.5253\n",
      "Epoch [8/100], Train Loss: 0.0226, Train Accuracy: 0.5005, Validation Loss: 0.0291, Validation Accuracy: 0.5347\n",
      "Epoch [9/100], Train Loss: 0.0227, Train Accuracy: 0.4963, Validation Loss: 0.0235, Validation Accuracy: 0.5334\n",
      "Epoch [10/100], Train Loss: 0.0227, Train Accuracy: 0.5014, Validation Loss: 0.0218, Validation Accuracy: 0.5343\n",
      "Epoch [11/100], Train Loss: 0.0227, Train Accuracy: 0.5010, Validation Loss: 0.0233, Validation Accuracy: 0.5141\n",
      "Epoch [12/100], Train Loss: 0.0227, Train Accuracy: 0.4985, Validation Loss: 0.0219, Validation Accuracy: 0.5292\n",
      "Epoch [13/100], Train Loss: 0.0227, Train Accuracy: 0.4984, Validation Loss: 0.0219, Validation Accuracy: 0.5303\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     36\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[1;32m---> 37\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m, in \u001b[0;36mDeepfakeDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     17\u001b[0m crop_tensors \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m crop_file \u001b[38;5;129;01min\u001b[39;00m crops_list:\n\u001b[1;32m---> 20\u001b[0m     loaded_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrop_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     crop_tensors\u001b[38;5;241m.\u001b[39mappend(loaded_tensor)\n\u001b[0;32m     23\u001b[0m crop_tensors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(crop_tensors, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# stack tensors. this is a \"batch\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "model = model.to(device)\n",
    "earlystopper = EarlyStopper()\n",
    "# mtcnn unable to detect gan/diffusion faces\n",
    "#data = data.to(device)  # Move data to GPU if available\n",
    "# half precision\n",
    "# A6000 at desktop\n",
    "for epoch in range(epochs): \n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for data, labels in train_loader:\n",
    "        data = data.squeeze(0)\n",
    "        labels = labels.squeeze(0)\n",
    "        labels = labels.to(device).float()\n",
    "        optimizer.zero_grad()  # clear previous gradients\n",
    "\n",
    "        data = data.to(device).float()\n",
    "        outputs = model(data)\n",
    "        losses = loss_fn(outputs, labels)\n",
    "        running_loss += losses.item()  # accumulate loss\n",
    "        predicted = (outputs >= 0.5).float() # calculate if label is 0 or 1\n",
    "        losses.backward()\n",
    "        correct += (predicted == labels).sum().item() \n",
    "        total += labels.size(0)\n",
    "\n",
    "    average_train_loss = running_loss / total\n",
    "    average_train_accuracy = correct / total\n",
    "    train_loss_history.append(average_train_loss)\n",
    "    train_accuracy_history.append(average_train_accuracy)\n",
    "\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0 \n",
    "    for data, labels in val_loader:\n",
    "        data = data.squeeze(0)\n",
    "        labels = labels.squeeze(0)\n",
    "        labels = labels.to(device).float()\n",
    "        data = data.to(device).float()\n",
    "\n",
    "        outputs = model(data)\n",
    "        losses = loss_fn(outputs, labels)\n",
    "        running_loss += losses.item()  # accumulate loss\n",
    "        predicted = (outputs >= 0.5).float() # calculate if label is 0 or 1\n",
    "        losses.backward()\n",
    "        correct += (predicted == labels).sum().item() \n",
    "        total += labels.size(0)\n",
    "    average_val_loss = running_loss / total\n",
    "    average_val_accuracy = correct / total\n",
    "    val_loss_history.append(average_val_loss)\n",
    "    val_accuracy_history.append(average_val_accuracy)\n",
    "    print(f\"Epoch [{epoch}/{epochs}], Train Loss: {average_train_loss:.4f}, Train Accuracy: {average_train_accuracy:.4f}, Validation Loss: {average_val_loss:.4f}, Validation Accuracy: {average_val_accuracy:.4f}\")\n",
    "    #if earlystopper.early_stop(average_val_loss, model):\n",
    "    #    break \n",
    "torch.save(model.state_dict(), 'model_dfdc.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_dfdc.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained_weights_path = 'noisy-student-efficientnet-b4.pth' # noisy student\n",
    "#model = EfficientNet.from_name('efficientnet-b4')\n",
    "#model.load_state_dict(torch.load(pretrained_weights_path, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
