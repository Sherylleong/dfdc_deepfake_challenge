{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm import tqdm_notebook\n",
    "%matplotlib inline \n",
    "# from google.colab.patches import cv2_imshow\n",
    "from IPython.display import HTML #imports to play videos\n",
    "from base64 import b64encode \n",
    "import cv2 as cv\n",
    "#from skimage.measure import compare_ssim\n",
    "import glob\n",
    "import time\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1, extract_face\n",
    "from tqdm import tqdm\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import math\n",
    "import pickle\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "import skimage.measure\n",
    "import albumentations as A\n",
    "from tqdm.notebook import tqdm \n",
    "#from albumentations.pytorch import ToTensor \n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models.video import mc3_18, r2plus1d_18\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "import sklearn\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import models, transforms\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "\n",
    "DATA_FOLDER = os.getcwd()\n",
    "TRAIN_SAMPLE_FOLDER = \"dfdc_train_part_1\"\n",
    "TEST_FOLDER = \"dfdc_train_part_0\"\n",
    "\n",
    "\n",
    "BASE_PATH = os.getcwd()\n",
    "\n",
    "# define the names of the training, testing, and validation\n",
    "# directories\n",
    "TRAIN = \"training\"\n",
    "TEST = \"evaluation\"\n",
    "VAL = \"validation\"\n",
    "\n",
    "REAL = 'REAL'\n",
    "FAKE = 'FAKE'\n",
    "\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "# initialize the list of class label names\n",
    "CLASSES = [\"FAKE\", \"REAL\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train metadata\n",
    "\n",
    "from glob import glob\n",
    "import json\n",
    "def get_videos_from_folder(folder_path):\n",
    "    # List all files in the specified folder\n",
    "    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    if 'metadata.json' in files:\n",
    "        files.remove('metadata.json')\n",
    "    full_paths = [os.path.join(folder_path, f) for f in files]\n",
    "    return full_paths\n",
    "\n",
    "def get_videos_basenames_from_folder(folder_path):\n",
    "    # List all files in the specified folder\n",
    "    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    if 'metadata.json' in files:\n",
    "        files.remove('metadata.json')\n",
    "    return files\n",
    "\n",
    "def get_original_with_fakes(root_dir):\n",
    "    pairs = []\n",
    "    for json_path in glob(os.path.join(root_dir, \"*/metadata.json\")):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "        for k, v in metadata.items():\n",
    "            original = v.get(\"original\", None)\n",
    "            if v[\"label\"] == \"FAKE\":\n",
    "                pairs.append((original[:-4], k[:-4] ))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "metadata_val = pd.read_json(TEST_FOLDER+ '/metadata.json')\n",
    "metadata_train = pd.read_json(TRAIN_SAMPLE_FOLDER+ '/metadata.json')\n",
    "#metadata['aaqaifqrwn.mp4']['label']\n",
    "\n",
    "def sample_weights(metadata):\n",
    "    video_fake_real_labels = metadata.iloc[0]\n",
    "    class_counts = video_fake_real_labels.value_counts()\n",
    "    class_weights = 1 / class_counts\n",
    "    return [1/class_counts[i] for i in video_fake_real_labels] # for undersampling\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "train_weights_sampler = WeightedRandomSampler(weights=sample_weights(metadata_train), num_samples=len(metadata_train.iloc[0]), replacement=True)\n",
    "val_weights_sampler =WeightedRandomSampler(weights=sample_weights(metadata_val), num_samples=len(metadata_val.iloc[0]), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set will be 00\n",
    "metadata_val = pd.read_json(TEST_FOLDER+ '/metadata.json')\n",
    "metadata_train = pd.read_json(TRAIN_SAMPLE_FOLDER+ '/metadata.json')\n",
    "video_fake_real_labels = metadata_train.iloc[0]\n",
    "real_df = video_fake_real_labels[video_fake_real_labels == 'REAL'].keys()\n",
    "fake_df = video_fake_real_labels[video_fake_real_labels == 'REAL'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:34: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n",
      "c:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:79: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n",
      "c:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:132: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "frames_per_video = 32\n",
    "mtcnn = MTCNN(margin=20, select_largest=False, factor=0.5, device=device, post_process=True) # post_process=False if want human readable\n",
    "img_size = 380\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "class ImageTransform:\n",
    "    def __init__(self, size, mean, std):\n",
    "        self.data_transform = transforms.Compose([\n",
    "                transforms.Resize((size, size), interpolation=Image.BILINEAR),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.data_transform(img)\n",
    "\n",
    "\n",
    "\n",
    "def extract_frames_from_video(path, n_frames=32):\n",
    "    # Create video reader and find length\n",
    "    v_cap = cv2.VideoCapture(path)\n",
    "    v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Pick 'n_frames' evenly spaced frames to sample\n",
    "    if n_frames is None:\n",
    "        sample = np.arange(0, v_len)\n",
    "    else:\n",
    "        sample = np.linspace(0, v_len - 1, n_frames).astype(int)\n",
    "    # Loop through frames\n",
    "    faces = []\n",
    "    frames = []\n",
    "    for j in range(v_len):\n",
    "        success = v_cap.grab()\n",
    "        if j in sample:\n",
    "            # Load frame\n",
    "            success, frame = v_cap.retrieve()\n",
    "            if not success:\n",
    "                continue\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "    return frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, metadata, device, detector, transform, mode='train'):\n",
    "        self.device = device\n",
    "        self.detector = detector\n",
    "        self.transform = transform\n",
    "        self.folder_path = TRAIN_SAMPLE_FOLDER if mode=='train' else TEST_FOLDER\n",
    "        self.video_fake_real_labels = metadata.iloc[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_fake_real_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.video_fake_real_labels.index[idx]\n",
    "        label = 0 if self.video_fake_real_labels[filename] == 'REAL' else 1\n",
    "        path = os.path.join(DATA_FOLDER, self.folder_path, filename)\n",
    "        img_list = []\n",
    "        # Movie to Image\n",
    "        try:\n",
    "            frames = extract_frames_from_video(path)\n",
    "        except:\n",
    "            return [], label\n",
    "        # Detect Faces\n",
    "        for frame in frames:\n",
    "            try:\n",
    "                _frame = frame[np.newaxis, :, :, :]\n",
    "                boxes, probs = self.detector.detect(_frame, landmarks=False)\n",
    "                x = int(boxes[0][0][0])\n",
    "                y = int(boxes[0][0][1])\n",
    "                z = int(boxes[0][0][2])\n",
    "                w = int(boxes[0][0][3])\n",
    "                image = frame[y-15:w+15, x-15:z+15]\n",
    "                \n",
    "                # Preprocessing\n",
    "                image = Image.fromarray(image)\n",
    "                image = self.transform(image)\n",
    "                \n",
    "                img_list.append(image)\n",
    "\n",
    "            except Exception as e:\n",
    "                img_list.append(None)\n",
    "            \n",
    "        # Padding None\n",
    "        img_list = [c for c in img_list if c is not None]\n",
    "        \n",
    "        return img_list, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained_weights_path = 'noisy-student-efficientnet-b4.pth' # noisy student\n",
    "#model = EfficientNet.from_name('efficientnet-b4')\n",
    "#model = EfficientNet.from_name('efficientnet-b1')\n",
    "#model.load_state_dict(torch.load(pretrained_weights_path, map_location=torch.device(device)))\n",
    "#model._fc = nn.Linear(in_features=model._fc.in_features, out_features=1)\n",
    "class EffnetTest(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EffnetTest, self).__init__()\n",
    "        self.model = EfficientNet.from_name(f\"efficientnet-b1\")\n",
    "        self.model._fc = nn.Linear(in_features=1280, out_features=1)\n",
    "        #self.model._norm_layer = nn.GroupNorm(num_groups=32, num_channels=3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return torch.sigmoid(x).squeeze()\n",
    "model = EffnetTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DeepfakeDataset(metadata_train, device, mtcnn, ImageTransform(img_size, mean, std), mode='train')\n",
    "train_loader = DataLoader(train_dataset, shuffle=False, sampler=train_weights_sampler, batch_size=1) # process video 1 by 1\n",
    "val_dataset = DeepfakeDataset(metadata_val, device, mtcnn, ImageTransform(img_size, mean, std), mode='train')\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, sampler=val_weights_sampler, batch_size=1) # process video 1 by 1\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "loss_history = []\n",
    "accuracy_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "model = model.to(device)\n",
    "#data = data.to(device)  # Move data to GPU if available\n",
    "for epoch in range(epochs): \n",
    "    print('epoch', epoch)\n",
    "    for data, label in train_loader:\n",
    "        model.train()  # set the model to training mode\n",
    "        optimizer.zero_grad()  # clear previous gradients\n",
    "        outputs=[]\n",
    "        for face in data:\n",
    "            face = face.to(device)\n",
    "            output = model(face)\n",
    "            outputs.append(output)\n",
    "        answer = torch.mean(torch.stack(outputs), dim=0)\n",
    "        loss = loss_fn(answer, label.squeeze().float())\n",
    "        loss.backward()\n",
    "       \n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained_weights_path = 'noisy-student-efficientnet-b4.pth' # noisy student\n",
    "#model = EfficientNet.from_name('efficientnet-b4')\n",
    "model = EfficientNet.from_name('efficientnet-b1')\n",
    "#model.load_state_dict(torch.load(pretrained_weights_path, map_location=torch.device(device)))\n",
    "model._fc = nn.Linear(in_features=model._fc.in_features, out_features=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
